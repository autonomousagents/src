\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}The Environment}{1}}
\newlabel{environment}{{1.1}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Simulating the environment}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Planning for the Environment}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1} Policy Evaluation}{2}}
\newlabel{eq:policyEvaluation}{{1}{2}}
\newlabel{eq:policyEvaluationSimpler}{{2}{2}}
\newlabel{list:policyEvaluationListValues}{{3.1}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}(SC) Policy Iteration}{2}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparison of iterations of Value Iteration and Policy Iteration\relax }}{2}}
\newlabel{tab:policyEvaluationValues}{{1}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{colormapPolicyEvaluation}{{\caption@xref {colormapPolicyEvaluation}{ on input line 77}}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Colormap of the $V$-values resulting from Policy Evaluation for $\theta =0$ and $\gamma = 0.8$. \newline  Each axis represents all tiles of the grid world. The brighter the color the higher the corresponding $V$-value.\relax }}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Value Iteration}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Colormap of the $V$-values resulting from Value Iteration for $\theta =0$ and $\gamma = 0.9$. \newline  The brighter the color the higher the corresponding $V$-value.\relax }}{4}}
\newlabel{colormapValueIteration}{{2}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces The $V$-values for Value Iteration, with $\gamma $ = 0.1 and the prey at position (5,5). The convergence speed is 20 iterations.\relax }}{5}}
\newlabel{valueiterationone}{{2}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces The $V$-values for Value Iteration, with $\gamma $ = 0.5 and the prey at position (5,5). The convergence speed is 28 iterations.\relax }}{5}}
\newlabel{valueiteration2}{{3}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces The $V$-values for Value Iteration, with $\gamma $ = 0.7 and the prey at position (5,5). The convergence speed is 31 iterations.\relax }}{6}}
\newlabel{valueiteration3}{{4}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces The $V$-values for Value Iteration, with $\gamma $ = 0.9 and the prey at position (5,5). The convergence speed is 34 iterations.\relax }}{6}}
\newlabel{valueiteration4}{{5}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {4}(SC) State space reduction}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Simulating the Environment}{8}}
\newlabel{app:firstMust}{{A}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Policy Evaluation}{9}}
\newlabel{app:firstShould}{{B}{9}}
