\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}The Environment}{1}}
\newlabel{environment}{{1.1}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Simulating the environment}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Results}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Planning for the Environment}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1} Policy Evaluation}{2}}
\newlabel{eq:policyEvaluation}{{1}{2}}
\newlabel{eq:policyEvaluationSimpler}{{2}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Results}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:policyEvaluation}{{\caption@xref {tab:policyEvaluation}{ on input line 88}}{2}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces State values using Policy Evaluation ($\theta = 0$, $\gamma = 0.8$)\relax }}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Policy Iteration}{2}}
\newlabel{sec:policyIteration}{{3.2}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Colormap of the $V$-values resulting from Policy Evaluation for $\theta =0$ and $\gamma = 0.8$. \newline  Each axis represents all tiles of the grid world. The brighter the color the higher the corresponding $V$-value.\relax }}{3}}
\newlabel{colormapPolicyEvaluation}{{1}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Policy Improvement}{3}}
\newlabel{policyImprovement}{{3.2.1}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Results}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Value Iteration}{4}}
\newlabel{sec:valueIteration}{{3.3}{4}}
\newlabel{valueIteration}{{4}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Results}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Colormap of the $V$-values resulting from Value Iteration for $\theta =0$ and $\gamma = 0.9$. \newline  The brighter the color the higher the corresponding $V$-value.\relax }}{4}}
\newlabel{colormapValueIteration}{{2}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Comparison of iterations of Value Iteration and Policy Iteration\relax }}{5}}
\newlabel{tab:policyEvaluationValues}{{2}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {4}(SC) State space reduction}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Simulating the Environment}{6}}
\newlabel{app:firstMust}{{A}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Policy Iteration and Value Iteration results}{8}}
\newlabel{app:values}{{B}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces The $V$-values for Value Iteration, with $\gamma $ = 0.1 and the prey at position (5,5). The convergence speed is 20 iterations.\relax }}{8}}
\newlabel{valueiterationone}{{\caption@xref {valueiterationone}{ on input line 337}}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces The $V$-values for Value Iteration, with $\gamma $ = 0.5 and the prey at position (5,5). The convergence speed is 28 iterations.\relax }}{8}}
\newlabel{valueiteration2}{{4}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces The $V$-values for Value Iteration, with $\gamma $ = 0.7 and the prey at position (5,5). The convergence speed is 31 iterations.\relax }}{8}}
\newlabel{valueiteration3}{{5}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces The $V$-values for Value Iteration, with $\gamma $ = 0.9 and the prey at position (5,5). The convergence speed is 34 iterations.\relax }}{9}}
\newlabel{valueiteration4}{{6}{9}}
