\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}\emph  {Assignment 1}: Single Agent Planning}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}(M) Simulating the Environment}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}(SC) State space reduction}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}(SC) Policy Evaluation}{2}}
\newlabel{list:policyEvaluationListValues}{{2.3}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{colormapPolicyEvaluation}{{\caption@xref {colormapPolicyEvaluation}{ on input line 65}}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Colormap of the $V$-values resulting from Policy Evaluation for $\theta =0$ and $\gamma = 0.8$. \newline  Each axis represents all tiles of the grid world. The brighter the color the higher the corresponding $V$-value.\relax }}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}(M) Value Iteration}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Colormap of the $V$-values resulting from Value Iteration for $\theta =0$ and $\gamma = 0.9$. \newline  The brighter the color the higher the corresponding $V$-value.\relax }}{3}}
\newlabel{colormapValueIteration}{{2}{3}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces The $V$-values for Value Iteration, with $\gamma $ = 0.1 and the prey at position (5,5). The convergence speed is 20 iterations.\relax }}{4}}
\newlabel{valueiterationone}{{1}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces The $V$-values for Value Iteration, with $\gamma $ = 0.5 and the prey at position (5,5). The convergence speed is 28 iterations.\relax }}{4}}
\newlabel{valueiteration2}{{2}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces The $V$-values for Value Iteration, with $\gamma $ = 0.7 and the prey at position (5,5). The convergence speed is 31 iterations.\relax }}{5}}
\newlabel{valueiteration3}{{3}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces The $V$-values for Value Iteration, with $\gamma $ = 0.9 and the prey at position (5,5). The convergence speed is 34 iterations.\relax }}{5}}
\newlabel{valueiteration4}{{4}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}(SC) Policy Iteration}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Comparison of iterations of Value Iteration and Policy Iteration\relax }}{6}}
\newlabel{tab:policyEvaluationValues}{{5}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Simulating the Environment}{7}}
\newlabel{app:firstMust}{{A}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Policy Evaluation}{8}}
\newlabel{app:firstShould}{{B}{8}}
