\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}The Environment}{1}}
\newlabel{environment}{{1.1}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Implementation details}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Simulating the environment}{1}}
\citation{russel}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Results}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Planning for the environment}{2}}
\newlabel{simpleStates}{{3}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1} Policy Evaluation}{2}}
\newlabel{eq:policyEvaluation}{{1}{2}}
\newlabel{eq:policyEvaluationSimpler}{{2}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Results}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:policyEvaluation}{{\caption@xref {tab:policyEvaluation}{ on input line 90}}{2}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces State values using Policy Evaluation ($\theta = 0$, $\gamma = 0.8$)\relax }}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Colormap of the $V$-values resulting from Policy Evaluation for $\theta =0$ and $\gamma = 0.8$. \newline  Each axis represents all tiles of the grid world. The brighter the color the higher the corresponding $V$-value.\relax }}{3}}
\newlabel{colormapPolicyEvaluation}{{1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Policy Iteration}{3}}
\newlabel{sec:policyIteration}{{3.2}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Policy Improvement}{3}}
\newlabel{policyImprovement}{{3.2.1}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Results}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Value Iteration}{4}}
\newlabel{sec:valueIteration}{{3.3}{4}}
\newlabel{valueIteration}{{4}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Results}{4}}
\newlabel{resultsValueIteration}{{3.3.1}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Comparison of iterations of Value Iteration and Policy Iteration (P.E. is Policy Evaluation)\relax }}{4}}
\newlabel{tab:policyEvaluationValues}{{2}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Colormap of the $V$-values resulting from Value Iteration for $\theta =0$ and $\gamma = 0.9$. \newline  The brighter the color the higher the corresponding $V$-value.\relax }}{5}}
\newlabel{colormapValueIteration}{{2}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {4}State space reduction}{5}}
\newlabel{stateSpace}{{4}{5}}
\citation{sutton}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Colormap of the $V$-values resulting from Policy Evaluation for $\theta =0$ and $\gamma = 0.8$ \newline  using the ``efficient'' state space representation. The brighter the color the higher the corresponding $V$-value. The prey is always located on the (1, 1) coordinate in this state representation.\relax }}{6}}
\newlabel{NewStateRep}{{3}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{6}}
\citation{*}
\bibstyle{plainnat}
\bibdata{references}
\@writefile{toc}{\contentsline {section}{\numberline {A}Simulating the Environment}{7}}
\newlabel{app:firstMust}{{A}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Policy Iteration and Value Iteration results}{9}}
\newlabel{app:values}{{B}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces The $V$-values for Value Iteration and Policy Iteration, with $\gamma $ = 0.1 and the prey at position \textbf  {(5,5)}.\relax }}{9}}
\newlabel{valueiterationone}{{3}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces The $V$-values for Value Iteration and Policy Iteration, with $\gamma $ = 0.5 and the prey at position \textbf  {(5,5)}. \relax }}{9}}
\newlabel{valueiteration2}{{4}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces The $V$-values for Value Iteration and Policy Iteration, with $\gamma $ = 0.7 and the prey at position \textbf  {(5,5)}. \relax }}{9}}
\newlabel{valueiteration3}{{5}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces The $V$-values for Value Iteration and Policy Iteration, with $\gamma $ = 0.9 and the prey at position \textbf  {(5,5)}.\relax }}{10}}
\newlabel{valueiteration4}{{6}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Policy Evaluation comparison}{11}}
\newlabel{app:polEvaluation}{{C}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}Default state space representation}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.2}Efficient state space representation}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {D}Class diagram}{15}}
\newlabel{app:classDiagram}{{D}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  A class diagram of our code. For clarity purposes, not all methods and attributes of the classes are shown.\relax }}{15}}
\newlabel{pic:classDiagram}{{4}{15}}
